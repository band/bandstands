---
created: 2022-11-20T17:47:00-06:00
modified: 2022-11-20T17:47:11-06:00
---

David Roberts (@drvolts): I've got a few things to say about all this SBF/FTX/EA mishegas, and I guess I might as well say them before this website dies. I want to pivot off this characteristically excellent piece from @EricLevitz.  https://nymag.com/intelligencer/2022/11/effective-altruism-sam-bankman-fried-sbf-ftx-crypto.html

David Roberts (@drvolts): Levitz's piece gets at the essential question (why not do what SBF did?) but then dances around & never quite lands on what I think is the most important answer to it. 

So. Start w/ the fact that EA is basically an attempt to put utilitarianism to use in philanthropy.

David Roberts (@drvolts): The idea is to do the most good per dollar spent. As many people (including @mattyglesias) have argued, this basic heuristic is sensible and badly needed in the philanthropic sector, where there's a *lot* of feel-good spending w/ mediocre results. So far so good.

David Roberts (@drvolts): The tricky part comes when you treat utilitarianism less as a kind of rough heuristic and more as a kind of logical, quasi-mathematical rule. As many many philosophers have pointed out over the years, that leads to some highly counter-intuitive results.

David Roberts (@drvolts): Some folks point to counter-intuitive results as a reason to rejected utilitarianism; others point to those results & say "tough shit, logic is logic." So you have to accept, eg, that you can't buy your kids nice things for xmas b/c the money would save more lives in Africa.

David Roberts (@drvolts): EA has recently run into a version of this dilemma. "Longtermism" is the idea that there will be *many more future lives* than there are lives today, so action to prevent, say, a small risk of extinction trumps action to address even large current problems.

David Roberts (@drvolts): Oh, I meant to say, even prior to longtermism, you have the more prosaic result that, on a pure utility basis, it's better to make a shitload of money & donate it to EA than it is to go work for some charity. Thus "earn to give."

David Roberts (@drvolts): So SBF was convinced that becoming a hedge fund dude & contributing to efforts to forestall the AI apocalypse is how he could maximize his own utility. He may even have convinced himself that being a ponzi scammer & giving the money to EA maximized his impact.

David Roberts (@drvolts): Now (sorry, all that was prelude), I would put it to you: IF he is correct that saving countless future people swamps other considerations, AND that AI is the biggest threat to the species, THEN he was correct that literally anything he did to make money for EA is permissible.

David Roberts (@drvolts): IOW, if his expected-value calculations were correct, then SBF was perfectly justified in doing what he did. After all, what is a few billion in losses for some billionaires relative to the possible existence & flourishing of 10s of billions of future humans? Nothing!

David Roberts (@drvolts): Now, most everybody wants to avoid this conclusion, including folks in EA. So McKaskill et al. spend a lot of time in Levitz's piece basically trying to refute SBF's expected-value calculations. "What if you're caught & your crime casts doubt on all of EA?" etc. etc.

David Roberts (@drvolts): That's the part I found frustrating, because to me the problem with SBF's calculations were less moral than *epistemic*. It's less about the substance of this particular equation than the general practice of trying to reason through large, complex systems over long time periods.

David Roberts (@drvolts): Two things:
1. Humans have radically limited information & have consistently proven AWFUL at predicting the future.
2. Humans are very, very, very, very good at bullshitting themselves, ie, "motivated reasoning" that leads to conclusions congenial to one's priors.

David Roberts (@drvolts): It follows that the bigger & more complex the systems you're reasoning about, and the farther out into the future your reasoning extends, the more likely you are to be wrong, & not just wrong, but wrong in ways that flatter your priors & identity.

David Roberts (@drvolts): I always feel like this fundamental fact gets underplayed in discussions of EA or various other "rationalist" communities. The tendency to bullshit oneself is basically ... undefeated. It gets everyone eventually, even the most self-disciplined of thinkers.

David Roberts (@drvolts): If we humans overcome this at all, it is not through individuals Reasoning Harder or learning lists of common logical fallacies or whatever. If we achieve reason at all (which is rarely), we do so *socially*, together, as communities of inquiry.

David Roberts (@drvolts): We grope toward reason & truth together, knowing that no individual is free of various epistemic weaknesses, but perhaps together, reviewing one another's work, pressing & challenging one another, adhering to shared epistemic standards, we can stumble a little closer.

David Roberts (@drvolts): That's what science is, insofar at it works -- not some isolated genius thinking really hard, but a *structured community of inquiry* that collectively zigs & zags its way in the right direction. Any one of us will almost certainly succumb to self-BSing. Together? Sometimes not.

David Roberts (@drvolts): The best an individual can do in this circumstance is struggle to maintain intellectual humility & "negative capacity" (the ability to sit in uncertainty w/out itching after resolution), as described in this lovely @willwilkinson post. https://modelcitizen.substack.com/p/before-truth-curiosity-negative-capability

David Roberts (@drvolts): Intellectual humility would suggest that, if our reasoning leads us to a place where we've justified ourselves acting in ways that violate most people's intuitions & produce proximate suffering, we should be *very, very suspicious that we have bullshitted ourselves*.
